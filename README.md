# DeepLearning-CIFAR10-Experiments 🚀

## Overview 📚
This repository is a playground for exploring various neural network models and techniques using the CIFAR-10 dataset within PyTorch. The series of Jupyter notebooks demonstrate how different configurations like learning rates, activation functions, and model complexities impact the performance of image classification models.

## Contents 📂
Here's a brief overview of each notebook in this repository, each tailored to experiment with specific aspects of neural network training:

- `Adam.ipynb` - 🔧 Utilizes the Adam optimizer for network training.
- `Cifar10_tutorial.ipynb` - 🏫 Basic introduction to CIFAR-10 classification.
- `Cifar10_tutorial_LargeLR.ipynb` - ⬆️ Investigates the effects of a larger learning rate.
- `Cifar10_tutorial_LeakyReLU.ipynb` - ⚡ Implements LeakyReLU to examine its impact on accuracy.
- `Cifar10_tutorial_SmallLR.ipynb` - ⬇️ Tests performance with a smaller learning rate.
- `Cifar10_tutorial_Tanh.ipynb` - 🌀 Explores the Tanh activation function.
- `Cifar10_tutorial_single_layer.ipynb` - 🧱 Builds a simple model with a single layer.
- `Cifar10_tutorial_three_layer.ipynb` - 🏗️ Designs a more complex model with three layers.
- `Larger_Batch_Size.ipynb` - 📦 Examines the impact of using larger batch sizes.
- `Longer_Training.ipynb` - ⏳ Analyzes the benefits of extending the training duration.
- `SGDOptimizer.ipynb` - 🔄 Trains the network using the Stochastic Gradient Descent optimizer.
- `change1_Cifar10_tutorial.ipynb` - ✨ First modification to the original CIFAR-10 tutorial.

## Technologies Used 💻
- **PyTorch**: Main deep learning library used.
- **Jupyter Notebook**: For interactive coding and visualization.

## Reference 📖
These tutorials are based on the [official PyTorch CIFAR-10 tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).

