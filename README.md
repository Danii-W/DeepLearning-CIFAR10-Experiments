# DeepLearning-CIFAR10-Experiments ğŸš€

## Overview ğŸ“š
This repository is a playground for exploring various neural network models and techniques using the CIFAR-10 dataset within PyTorch. The series of Jupyter notebooks demonstrate how different configurations like learning rates, activation functions, and model complexities impact the performance of image classification models.

## Contents ğŸ“‚
Here's a brief overview of each notebook in this repository, each tailored to experiment with specific aspects of neural network training:

- `Adam.ipynb` - ğŸ”§ Utilizes the Adam optimizer for network training.
- `Cifar10_tutorial.ipynb` - ğŸ« Basic introduction to CIFAR-10 classification.
- `Cifar10_tutorial_LargeLR.ipynb` - â¬†ï¸ Investigates the effects of a larger learning rate.
- `Cifar10_tutorial_LeakyReLU.ipynb` - âš¡ Implements LeakyReLU to examine its impact on accuracy.
- `Cifar10_tutorial_SmallLR.ipynb` - â¬‡ï¸ Tests performance with a smaller learning rate.
- `Cifar10_tutorial_Tanh.ipynb` - ğŸŒ€ Explores the Tanh activation function.
- `Cifar10_tutorial_single_layer.ipynb` - ğŸ§± Builds a simple model with a single layer.
- `Cifar10_tutorial_three_layer.ipynb` - ğŸ—ï¸ Designs a more complex model with three layers.
- `Larger_Batch_Size.ipynb` - ğŸ“¦ Examines the impact of using larger batch sizes.
- `Longer_Training.ipynb` - â³ Analyzes the benefits of extending the training duration.
- `SGDOptimizer.ipynb` - ğŸ”„ Trains the network using the Stochastic Gradient Descent optimizer.
- `change1_Cifar10_tutorial.ipynb` - âœ¨ First modification to the original CIFAR-10 tutorial.

## Technologies Used ğŸ’»
- **PyTorch**: Main deep learning library used.
- **Jupyter Notebook**: For interactive coding and visualization.

## Reference ğŸ“–
These tutorials are based on the [official PyTorch CIFAR-10 tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).

